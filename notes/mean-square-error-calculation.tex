\documentclass[11pt]{article}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{epstopdf}
\usepackage{tensor}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\DeclareMathOperator{\Tr}{Tr}

\title{Notes on the MSE calculation}
\author{Jeffrey J. Early}
\date{October 16, 2018---updated January 31st, 2019}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
%\section{}
%\subsection{}

Using our definitions,
\begin{align}
    \textrm{MSE}(\lambda) =& \frac{1}{N} || \left( \mathbf{S_\lambda} - I \right) \mathbf{x} ||^2 + \frac{2 \sigma^2}{N}  \Tr \mathbf{S_\lambda} - \sigma^2 \\
    =&  \left(1-\frac{1}{d_{\textrm{var}}} \right)\sigma^2 + \frac{2 \sigma^2}{d_\textrm{mean}} - \sigma^2 \\
    =& \sigma^2 \left( \frac{2}{d_\textrm{mean}} -\frac{1}{d_{\textrm{var}}} \right)
\end{align}

If we can assume that the DOF are the same, then we find that,
\begin{equation}
    \textrm{MSE} = \frac{\sigma^2}{d}
\end{equation}
which is a fairly intuitive result.

Key to note here is that the tension parameter in terms of degrees of freedom, has a minimum of $d=1$, and starts to asymptote much past $d=10$. So the range of tension parameters to consider is actually quite small.

Ultimately we want to reduce the MSE of all order of the spline fit. If the higher order information is all garbage, then we should know that.

Roughly speaking, this means we simply compute the MSE for velocity, acceleration, etc.

\begin{align}
    \textrm{MSE}(\lambda) =& \frac{1}{N} || \mathbf{S_\lambda}\mathbf{x} - \mathbf{x}_{\textrm{true}} ||^2 \\
    =& \frac{1}{N} \left[ \mathbf{S}\indices{^i_j} x^j - g^i \right]^2 \\
    =& \frac{1}{N} \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j + \mathbf{S}\indices{^i_j} \epsilon^j \right]^2 \\
    =& \frac{1}{N} \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j + \mathbf{S}\indices{^i_j} \epsilon^j \right]^2 \\ \nonumber
    =& \frac{1}{N} \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j  \right]^2  + \frac{1}{N} \left[ \mathbf{S}\indices{^i_j} \epsilon^j \right]^2\\
    & +\frac{2}{N} \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j  \right] \left[ \mathbf{S}\indices{^i_j} \epsilon^j \right]
\end{align}
Consider,
\begin{align}
    \left[\mathbf{S}\indices{^i_j} \epsilon^j\right]^2 =& \mathbf{S}\indices{^i_j} \epsilon^j \mathbf{S}\indices{^i_k} \epsilon^k \\
    =& \mathbf{S}\indices{^i_j}\mathbf{S}\indices{^i_k} \epsilon^j \epsilon^k \\
    =& \mathbf{S}\indices{^i_j}\mathbf{S}\indices{^i_j} \epsilon^j \epsilon^j \\
    =& \sigma^2 \mathbf{S}\indices{^i_j}\mathbf{S}\indices{^i_j} \\
    =& E\left[\sum_i \left( \mathbf{S}\indices{^i_j} \epsilon^j \right)^2 \right]
\end{align}
So this is the sum of the square of all the components in the matrix, which is the the trace of the of the matrix.
Now, under expectation we have that,
\begin{align}
    \textrm{MSE}(\lambda) =& \frac{1}{N} \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j  \right]^2  + \frac{\sigma^2}{N} \mathbf{S}\indices{^i_j} \mathbf{S}\indices{^j_i} \\
    =& \frac{1}{N} \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) \left(x^j-\epsilon^j\right)  \right]^2  + \frac{\sigma^2}{N} \mathbf{S}\indices{^i_j} \mathbf{S}\indices{^j_i}
\end{align}

If we include a derivative in there,
\begin{align}
    \textrm{MSE}(\lambda) =& \frac{1}{N} \left[ D\indices{^k_i} \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j  \right]^2  + \frac{1}{N} \left[ D\indices{^k_i} \mathbf{S}\indices{^i_j} \epsilon^j \right]^2\\
    & +\frac{2}{N} \left[ D\indices{^k_i} \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j  \right] \left[ D\indices{^k_i} \mathbf{S}\indices{^i_j} \epsilon^j \right]
\end{align}
The last term vanishes under expectation, as usual. The second to last term looks the same as before, in the sense that the matrix operation and the derivative can be combined, and so nothing really changed.
\begin{align}
    \textrm{MSE}(\lambda) =& \frac{1}{N} \left[ D\indices{^k_i} \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j  \right]^2  + \frac{1}{N} \left[ D\indices{^k_i} \mathbf{S}\indices{^i_j} \epsilon^j \right]^2 \\ \nonumber
    =& \frac{1}{N} \left[ D\indices{^k_i} \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) \left(x^j-\epsilon^j\right)  \right]^2  + \frac{1}{N} \left[ D\indices{^k_i} \mathbf{S}\indices{^i_j} \epsilon^j \right]^2\\
    =& \frac{1}{N}\left[ A\indices{^k_j} \left(x^j-\epsilon^j\right) \right]^2 + \frac{1}{N} \left[ D\indices{^k_i} \mathbf{S}\indices{^i_j} \epsilon^j \right]^2 \\
    =& \frac{1}{N}\left[ A\indices{^k_j} x^j-A\indices{^k_j}\epsilon^j \right]^2 + \frac{1}{N} \left[ D\indices{^k_i} \mathbf{S}\indices{^i_j} \epsilon^j \right]^2 \\
    =& \left[ A\indices{^k_j} x^j\right]^2 + \left[ A\indices{^k_j}\epsilon^j \right]^2 - 2 \left[ A\indices{^k_j} x^j\right]\left[ A\indices{^k_j}\epsilon^j \right] \\ \nonumber
    & +  \left[ D\indices{^k_i} \mathbf{S}\indices{^i_j} \epsilon^j \right]^2
\end{align}

Ultimately I get that,
\begin{equation}
    MSE(\lambda) = || D(I-S)x ||^2 - \sigma^2 \Tr \left(D - DS \right)^2 + \sigma^2 \Tr (DS)^2 
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%
%
\section{Unknown error} \label{sec:unknown_error}
%
%%%%%%%%%%%%%%%%%%%%%%

Just eliminate $d$ from the sample variance and sample mean equations, so that,
\begin{align}
    \left( 1 - \frac{1}{N} \Tr \left( \mathbf{S_\lambda} \right) \right) \sigma^2 =  \frac{1}{N} || \left( \mathbf{I} - \mathbf{S_\lambda} \right) \mathbf{x} ||^2
\end{align}
or
\begin{equation}
    \sigma^2 = \frac{ \frac{1}{N}|| \left( \mathbf{I} - \mathbf{S_\lambda} \right) \mathbf{x} ||^2}{1-\frac{1}{N}\Tr \left( \mathbf{S_\lambda} \right)}
\end{equation}
Now the MSE becomes,
\begin{align}
\nonumber
    \textrm{MSE}(\lambda) =& \frac{1}{N} || \left( \mathbf{S_\lambda} - I \right) \mathbf{x} ||^2 \\
    & + \left( \frac{\frac{1}{N}|| \left( \mathbf{I} - \mathbf{S_\lambda} \right) \mathbf{x} ||^2}{1-\frac{1}{N}\Tr \left( \mathbf{S_\lambda} \right)} \right) \left( \frac{2}{N} \Tr \mathbf{S_\lambda} -1 \right) \\
    =& \frac{1}{N}|| \left( \mathbf{I} - \mathbf{S_\lambda} \right) \mathbf{x} ||^2 \left( \frac{ \frac{1}{N} \Tr \mathbf{S_\lambda}}{1-\frac{1}{N}\Tr \left( \mathbf{S_\lambda} \right)} \right)
\end{align}

Alternatively, if we normalize the MSE by $\sigma^2$, then
\begin{align}
   \frac{1}{\sigma^2} \textrm{MSE}(\lambda) =& \frac{1}{N}\Tr \left( \mathbf{S_\lambda} \right)
\end{align}
which, duh, should have been obvious from my earlier calculation that the MSE is $\sigma^2/d$.

If the error $\sigma$ is not known, we can estimate it. For each value of sigma.

At one extreme the signal could be all noise
\begin{equation}
    \sigma_{\textrm{max}} = \sqrt{\frac{1}{N} \sum (x_i-\bar{x})^2}
\end{equation}
and thus our total degrees of freedom is $d=N$. At the other extreme, the noise is effectively zero, in that it is much less than the mean distance traversed,
\begin{equation}
    \sigma_{\textrm{min}} = \frac{1}{50} \sqrt{\frac{1}{N-1} \sum (x_i-x_{i-1})^2}
\end{equation}
which is equivalent to setting $\Gamma = 0.1$

%%%%%%%%%%%%%%%%%%%%%%
%
\section{MSE calculation revisited} \label{sec:mse_revisisted}
%
%%%%%%%%%%%%%%%%%%%%%%

I want to revisit the MSE calculation with the idea that the noise has a known and an unknown part.

\begin{align}
    N \cdot \textrm{MSE}(\lambda) =& || \mathbf{S_\lambda}\mathbf{x} - \mathbf{x}_{\textrm{true}} ||^2 \\
    =&  \left[ \mathbf{S}\indices{^i_j} x^j - g^i \right]^2 \\
    =&  \left[ \mathbf{S}\indices{^i_j} \left( g^j + \epsilon^j \right) - g^i \right]^2 \\
    =&  \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j + \mathbf{S}\indices{^i_j} \epsilon^j \right]^2 \\
    =& \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j \right]^2 + 2 \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j \right]  \left[ \mathbf{S}\indices{^i_j} \epsilon^j \right]+ \left[ \mathbf{S}\indices{^i_j} \epsilon^j \right]^2
\end{align}
That middle term will vanish under expectation. But, before we do that, let's expand the first term, that is,
\begin{align}
\left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j \right]^2  =& \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) (x^j-\epsilon^j) \right]^2 \\
=&  \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)x^j \right]^2 - 2 \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)x^j \right] \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)\epsilon^j \right] + \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)\epsilon^j \right]^2 \\
=&  \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)x^j \right]^2 + \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) \left( \epsilon^j - 2 x^j \right) \right] \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)\epsilon^j \right] \\
=&  \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)x^j \right]^2 + \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) \left( - \epsilon^j - 2 g^j \right) \right] \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)\epsilon^j \right] \\
=& \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)x^j \right]^2 - \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)\epsilon^j \right]^2 - 2 \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j \right] \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)\epsilon^j \right] 
\end{align}
Adding this back in to the original calculation, we end up with
\begin{align}
    N \cdot \textrm{MSE}(\lambda)  =& \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j \right]^2 + 2 \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j \right]  \left[ \mathbf{S}\indices{^i_j} \epsilon^j \right]+ \left[ \mathbf{S}\indices{^i_j} \epsilon^j \right]^2 \\
    =&  \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)x^j \right]^2 - \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)\epsilon^j \right]^2 + \left[ \mathbf{S}\indices{^i_j} \epsilon^j \right]^2 - 2 \left[ \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right) g^j \right] \epsilon^i
\end{align}
Now, under expectation, two things happen: the squared operator on random variable terms become the trace of that operator, and the random variable on deterministic term becomes zero. So, in total we have that
\begin{align}
    N \cdot \textrm{MSE}(\lambda)  =& || \left(\mathbf{S}\indices{^i_j} -\mathbf{I}\indices{^i_j} \right)x^j ||^2 - \sigma^2 \Tr \left(S - I\right)^2 + \sigma^2 \Tr S^2 \\
    \textrm{MSE}(\lambda) =& \frac{1}{N} || \left( \mathbf{S_\lambda} - I \right) \mathbf{x} ||^2 - \sigma^2 + \frac{2 \sigma^2}{N}  \Tr \mathbf{S_\lambda}
\end{align}
Now let's assume that our noise looks like this,
\begin{equation}
\label{robust_pdf}
    p_{\textrm{robust}}(\epsilon) = (1-\alpha) \cdot p_{\textrm{noise}}(\epsilon) + \alpha \cdot  p_{\textrm{outlier}}(\epsilon).
\end{equation}
where it's divided into two added components So now let's say we magically know which observations
\begin{align}
    (N_o + N_n) \cdot  \textrm{MSE}(\lambda) =& || \left( \mathbf{S^n_\lambda} - I \right) \mathbf{x} ||^2 - N_n \sigma_n^2  + 2  \sigma_n^2  \Tr \mathbf{S^n_\lambda} \\
    & || \left( \mathbf{S^o_\lambda} - I \right) \mathbf{x} ||^2 -  N_o \sigma_o^2 + 2 \sigma_o^2  \Tr \mathbf{S^o_\lambda}
\end{align}
Which is
\begin{align}
    (N_o + N_n) \cdot  \textrm{MSE}(\lambda) =& N_n \left(1-\frac{1}{n^n_\textrm{eff}} \right) - N_n \sigma_n^2  + 2 N_n \frac{\sigma_n^2}{n^n_\textrm{eff}} \\
    & || \left( \mathbf{S^o_\lambda} - I \right) \mathbf{x} ||^2 -  N_o \sigma_o^2 + 2 \sigma_o^2  \Tr \mathbf{S^o_\lambda}
    \end{align}
    which is likely just,
    \begin{align}
    (N_o + N_n) \cdot  \textrm{MSE}(\lambda) =& N_n \frac{\sigma_n^2}{n^n_\textrm{eff}} + N_o \frac{\sigma_o^2}{n^o_\textrm{eff}}
    \end{align}
Using that,
\begin{equation}
n_{\textrm{eff}} = C \cdot \left(\frac{\sigma}{u_{\textrm{rms}}\Delta t}\right)^m
\end{equation}
The $\Delta t$ changes in proportion to the number of each set of points, $\Delta t = \frac{T}{N}$, so
\begin{equation}
n_{\textrm{eff}} = C \cdot \left(\frac{\sigma N}{u_{\textrm{rms}}T}\right)^m
\end{equation}
Inserting this into our equation for the MSE,
    \begin{align}
 \textrm{MSE}(\lambda) =&\frac{ \left(u_{\textrm{rms}}T\right)^m}{N_o + N_n} \left( N_n \frac{\sigma_n^2}{ \left( \sigma_n N_n \right)^m } + N_o \frac{\sigma_o^2}{ \left( \sigma_o N_o \right)^m} \right) \\
 =& \left(u_{\textrm{rms}}T\right)^m\left( (1-\alpha) \frac{\sigma_n^{2-m}}{  (N_n)^m } + \alpha \frac{\sigma_o^{2-m}}{  (N_o)^m} \right) \\
  =& \left(\frac{u_{\textrm{rms}}T}{N}\right)^m\left( \frac{\sigma_n^{2-m}}{  (1-\alpha)^{m-1} } + \frac{\sigma_o^{2-m}}{  (\alpha)^{m-1}} \right)
    \end{align}
    
    Lets say we had equal parts outlier and noise, so $2N$ total points.
        \begin{align}
 \textrm{MSE}(\lambda) =&\frac{ \left(u_{\textrm{rms}}T\right)^m}{2} \left(  \frac{\sigma_n^2}{ \left( \sigma_n N \right)^m } +  \frac{\sigma_o^2}{ \left( \sigma_o N \right)^m} \right)
    \end{align}
    The problem here is that our effective sample size is not additive... so this doesn't reduce to the right value.
   
\end{document}  